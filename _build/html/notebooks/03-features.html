<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Getting word embeddings &#8212; podcast-ecog-ds  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training and evaluating encoding models" href="04-encoding.html" />
    <link rel="prev" title="ECoG quality checks" href="01-qualitychecks.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="Getting-word-embeddings">
<h1>Getting word embeddings<a class="headerlink" href="#Getting-word-embeddings" title="Link to this heading">¶</a></h1>
<p>This tutorial introduces how to extract features, or word embeddings based on our stimulus transcript. Features are numeric vectors that capture the meaning of the words in our transcript. Here, we will present two types types of features: interpretable syntactic features and high-dimensional word embeddings from a language model.</p>
<p>Acknowledgments: This tutorial draws heavily on the <a class="reference external" href="https://github.com/snastase/encling-tutorial/blob/main/encling_tutorial.ipynb">encling tutorial</a> by Samuel A. Nastase.</p>
<p>First, we’ll import some general-purpose Python packages.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
<section id="Extracting-syntactic-features">
<h2>Extracting syntactic features<a class="headerlink" href="#Extracting-syntactic-features" title="Link to this heading">¶</a></h2>
<p>One type of linguistic features are explicit grammatical features we are familiar with and have names for. These can include parts of speech (e.g., noun, verb) or syntactic dependencies (e.g., root, subject, object). We will use the <a class="reference external" href="https://github.com/explosion/spaCy">spaCy</a> library (Honnibal et al., 2020).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
</pre></div>
</div>
</div>
<p>First we need to load the transcript as a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> dataframe. It contains columns of words and their start and end timestamps.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../../monkey/stimuli/monkey_transcript.csv&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
    </tr>
    <tr>
      <th>1</th>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
    </tr>
    <tr>
      <th>3</th>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
    </tr>
    <tr>
      <th>5</th>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
    </tr>
    <tr>
      <th>6</th>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
    </tr>
    <tr>
      <th>7</th>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
    </tr>
    <tr>
      <th>8</th>
      <td>some</td>
      <td>6.892</td>
      <td>7.052</td>
    </tr>
    <tr>
      <th>9</th>
      <td>places</td>
      <td>7.072</td>
      <td>7.342</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>SpaCy requires us to download and load a model that enables its features. First, we will load the <a class="reference external" href="https://spacy.io/models/en#en_core_web_lg">en-core-web-lg</a> model, a large model trained on English and includes components for part-of-speech tagging and dependency parsing. If you did not download the model as part of the “Getting started” tutorial, uncomment the next cell and run it.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !python -m spacy download en_core_web_lg</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelname</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_lg&quot;</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">modelname</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Collecting en-core-web-lg==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">400.7/400.7 MB</span> <span class="ansi-red-fg">105.5 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
<span class="ansi-green-fg">✔ Download and installation successful</span>
You can now load the package via spacy.load(&#39;en_core_web_lg&#39;)
</pre></div></div>
</div>
<p>Language processing pipelines typically use a <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> to standarize the (sub-)word units (called tokens) they can operate on. Some words and punctuation will get separated into multiple tokens. For example, the word “there’s” will be tokenized into “there” and “‘s”. Thus the first step for us is to transform our transcript words into tokens that spaCy can work with.</p>
<p>To keep track of our word and their indices we first create a <code class="docutils literal notranslate"><span class="pre">word_idx</span></code> column. We then tokenize the words using the tokenizer. Then, we will <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html">explode</a> the dataframe so that each row of the dataframe is a token (and not a word). Note that we will add white spaces to the end of words before tokenization so we can track the boundary of each word. Compare the dataframes from before and from below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;word_idx&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;word_with_ws&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">word</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;hftoken&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">word_with_ws</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">explode</span><span class="p">(</span><span class="s2">&quot;hftoken&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word_idx</th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
      <th>word_with_ws</th>
      <th>hftoken</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
      <td>Act</td>
      <td>Act</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>one,</td>
      <td>one</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>one,</td>
      <td>,</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
      <td>monkey</td>
      <td>monkey</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
      <td>in</td>
      <td>in</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4</td>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
      <td>the</td>
      <td>the</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>middle.</td>
      <td>middle</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>middle.</td>
      <td>.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
      <td>So</td>
      <td>So</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7</td>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
      <td>there's</td>
      <td>there</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Now we will create a <a class="reference external" href="https://spacy.io/api/doc">doc</a> objcet (essentially a list of token objects) from our tokenized text:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">hftoken</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="n">spaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">whitespace_</span> <span class="o">==</span> <span class="s2">&quot; &quot;</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">hftoken</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">tokens</span><span class="o">.</span><span class="n">Doc</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">,</span> <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> <span class="n">spaces</span><span class="o">=</span><span class="n">spaces</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We will loop through the doc, and get the features for each token. The <a class="reference external" href="https://spacy.io/usage/linguistic-features#pos-tagging">features</a> include <code class="docutils literal notranslate"><span class="pre">text</span></code>, <code class="docutils literal notranslate"><span class="pre">tag</span></code> (detailed part-of-speech tag), <code class="docutils literal notranslate"><span class="pre">dep</span></code> (syntactic dependency, i.e. the relation between tokens), and <code class="docutils literal notranslate"><span class="pre">is_stop</span></code> (is the token part of a stop list, i.e. the most common words of the language?). We will organize the features into a second dataframe and add those columns back to <code class="docutils literal notranslate"><span class="pre">df</span></code>. We will drop the two columns we don’t need
anymore, and then save <code class="docutils literal notranslate"><span class="pre">df</span></code> for future encoding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">])</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">features</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="s2">&quot;dep&quot;</span><span class="p">,</span> <span class="s2">&quot;stop&quot;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span>
    <span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;hftoken&quot;</span><span class="p">,</span> <span class="s2">&quot;word_with_ws&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word_idx</th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
      <th>token</th>
      <th>pos</th>
      <th>dep</th>
      <th>stop</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
      <td>Act</td>
      <td>NNP</td>
      <td>ROOT</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>one</td>
      <td>CD</td>
      <td>nummod</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>,</td>
      <td>,</td>
      <td>punct</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
      <td>monkey</td>
      <td>NN</td>
      <td>appos</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
      <td>in</td>
      <td>IN</td>
      <td>prep</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4</td>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
      <td>the</td>
      <td>DT</td>
      <td>det</td>
      <td>True</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>middle</td>
      <td>NN</td>
      <td>pobj</td>
      <td>False</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>.</td>
      <td>.</td>
      <td>punct</td>
      <td>False</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
      <td>So</td>
      <td>RB</td>
      <td>advmod</td>
      <td>True</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7</td>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
      <td>there</td>
      <td>EX</td>
      <td>expl</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Since the features we extracted are all categorical, we need to turn them into numerical vectors. We will use <a class="reference external" href="https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelBinarizer.html">LabelBinarizer</a> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, which fits to all the possible category labels for a feature and then transforms our labels into one-hot binary vectors. There are 50 possible labels for <code class="docutils literal notranslate"><span class="pre">tag</span></code> and 45 possible for <code class="docutils literal notranslate"><span class="pre">dep</span></code>. So those two features will be turned into 50-dimensional and
45-dimensional vectors respectively. Our <code class="docutils literal notranslate"><span class="pre">is_stop</span></code> feature is binary, so it will just be one dimensional. We concatenate all three features to form a 96-dimensional syntactic feature overall and save it for future encoding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">taggerEncoder</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">get_pipe</span><span class="p">(</span><span class="s2">&quot;tagger&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
<span class="n">dependencyEncoder</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">get_pipe</span><span class="p">(</span><span class="s2">&quot;parser&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">taggerEncoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">dependencyEncoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dep</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">stop</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embeddings have a shape of: </span><span class="si">{</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Embeddings have a shape of: (5305, 96)
</pre></div></div>
</div>
</section>
<section id="Extracting-GPT-2-Features">
<h2>Extracting GPT-2 Features<a class="headerlink" href="#Extracting-GPT-2-Features" title="Link to this heading">¶</a></h2>
<p>Now we will extract contextual word embeddings from an autoregressive (or “causal”) large language model (LLM) called GPT-2 (<a class="reference external" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Radford et al., 2019</a>). GPT-2 relies on the Transformer architecture to sculpt the embedding of a given word based on the preceding context. The model is composed of a repeated circuit motif—called the “attention head”—by which the model can “attend” to previous words
in the context window when determining the meaning of the current word. This GPT-2 implementation is composed of 12 layers, each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 768 features and the context window includes the preceding 1024 tokens. Note that certain words will be broken up into multiple tokens; we’ll need to use GPT-2’s “tokenizer” to convert words into the appropriate
tokens. GPT-2 has been (pre)trained on large corpora of text according to a simple self-supervised objective function: predict the next word based on the prior context.</p>
<p>We will be using the <a class="reference external" href="https://huggingface.co">HuggingFace</a> <a class="reference external" href="https://huggingface.co/docs/transformers/index">transformers</a> library for working with these models. If you want to learn more about LLMs and GPT-2, here are some great blogs explaining <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">transformers</a> and <a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">GPT-2</a> architecture. The HuggingFace website also has many useful resources.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span><span class="p">,</span> <span class="n">find_executable_batch_size</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</pre></div>
</div>
</div>
<p>Let’s reload the stimulus transcript.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../../monkey/stimuli/monkey_transcript.csv&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
    </tr>
    <tr>
      <th>1</th>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
    </tr>
    <tr>
      <th>2</th>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
    </tr>
    <tr>
      <th>3</th>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
    </tr>
    <tr>
      <th>5</th>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
    </tr>
    <tr>
      <th>6</th>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
    </tr>
    <tr>
      <th>7</th>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
    </tr>
    <tr>
      <th>8</th>
      <td>some</td>
      <td>6.892</td>
      <td>7.052</td>
    </tr>
    <tr>
      <th>9</th>
      <td>places</td>
      <td>7.072</td>
      <td>7.342</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We will define some of the general arguments, including the model name as it appears on HuggingFace, the context length (i.e., how many tokens we input into the model), and compute device. We can set the device to <code class="docutils literal notranslate"><span class="pre">cuda</span></code> to utilize a GPU if it’s available.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelname</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">context_len</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We will now load the GPT-2 tokenizer to convert words into a list of tokens. Then, we will <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html">explode</a> the dataframe so that each row of the dataframe is a token. We will convert tokens to <code class="docutils literal notranslate"><span class="pre">token_ids</span></code> (integers IDs corresponding to words in the GPT-2 vocabulary, which contains approximately 50,000 tokens) to use as input into GPT-2.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">modelname</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;word_idx&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;hftoken&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">word</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">x</span><span class="p">))</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">explode</span><span class="p">(</span><span class="s2">&quot;hftoken&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;token_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">hftoken</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word_idx</th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
      <th>hftoken</th>
      <th>token_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
      <td>ĠAct</td>
      <td>2191</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>Ġone</td>
      <td>530</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>,</td>
      <td>11</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
      <td>Ġmonkey</td>
      <td>21657</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
      <td>Ġin</td>
      <td>287</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4</td>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
      <td>Ġthe</td>
      <td>262</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>Ġmiddle</td>
      <td>3504</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>.</td>
      <td>13</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
      <td>ĠSo</td>
      <td>1406</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7</td>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
      <td>Ġthere</td>
      <td>612</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Then we will download and load the pretrained GPT-2 model. You can inspect its configurations in <code class="docutils literal notranslate"><span class="pre">model.config</span></code> for more detailed information (e.g., number of layers, max context length).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading model...&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">modelname</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Model : </span><span class="si">{</span><span class="n">modelname</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Layers: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">EmbDim: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Config: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loading model...
Model : gpt2
Layers: 12
EmbDim: 768
Config: GPT2Config {
  &#34;_name_or_path&#34;: &#34;gpt2&#34;,
  &#34;activation_function&#34;: &#34;gelu_new&#34;,
  &#34;architectures&#34;: [
    &#34;GPT2LMHeadModel&#34;
  ],
  &#34;attn_pdrop&#34;: 0.1,
  &#34;bos_token_id&#34;: 50256,
  &#34;embd_pdrop&#34;: 0.1,
  &#34;eos_token_id&#34;: 50256,
  &#34;initializer_range&#34;: 0.02,
  &#34;layer_norm_epsilon&#34;: 1e-05,
  &#34;model_type&#34;: &#34;gpt2&#34;,
  &#34;n_ctx&#34;: 1024,
  &#34;n_embd&#34;: 768,
  &#34;n_head&#34;: 12,
  &#34;n_inner&#34;: null,
  &#34;n_layer&#34;: 12,
  &#34;n_positions&#34;: 1024,
  &#34;reorder_and_upcast_attn&#34;: false,
  &#34;resid_pdrop&#34;: 0.1,
  &#34;scale_attn_by_inverse_layer_idx&#34;: false,
  &#34;scale_attn_weights&#34;: true,
  &#34;summary_activation&#34;: null,
  &#34;summary_first_dropout&#34;: 0.1,
  &#34;summary_proj_to_labels&#34;: true,
  &#34;summary_type&#34;: &#34;cls_index&#34;,
  &#34;summary_use_proj&#34;: true,
  &#34;task_specific_params&#34;: {
    &#34;text-generation&#34;: {
      &#34;do_sample&#34;: true,
      &#34;max_length&#34;: 50
    }
  },
  &#34;transformers_version&#34;: &#34;4.45.2&#34;,
  &#34;use_cache&#34;: true,
  &#34;vocab_size&#34;: 50257
}

</pre></div></div>
</div>
<p>Since our transcript contains more tokens than the context window (32), we will reformat all the <code class="docutils literal notranslate"><span class="pre">token_ids</span></code> into <code class="docutils literal notranslate"><span class="pre">data</span></code>, a torch tensor with a shape of (number of tokens x 33). This is because to extract feature for a token from GPT-2 using context length 32, we will need to input 33 tokens to GPT-2, which contains the token itself and the 32 preceding tokens. Note that for the first 32 tokens in the transcript, we will use the pad_token_id or 0 to pad the input length to 33.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">token_id</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">fill_value</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fill_value</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">),</span> <span class="n">context_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fill_value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)):</span>
    <span class="n">example_tokens</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">context_len</span><span class="p">)</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">example_tokens</span><span class="p">)</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">example_tokens</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data has a shape of: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Data has a shape of: torch.Size([5491, 33])
</pre></div></div>
</div>
<p>We will use <a class="reference external" href="https://github.com/huggingface/accelerate">Accelerator</a> to make extracting features more efficient. It includes a <a class="reference external" href="https://huggingface.co/docs/accelerate/v0.11.0/en/memory">find_executable_batch_size</a> algorithm, which can find the optimal batch size for the code by decreasing the batch size in half after each failed run on the code (in this case, our <code class="docutils literal notranslate"><span class="pre">inference_loop</span></code> function).</p>
<p>Inside the <code class="docutils literal notranslate"><span class="pre">inference_loop</span></code> funcion, we will use a PyTorch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> to supply token IDs to the model in batches and extract the features. In addition to the embeddings, we’ll also extract several other features of potential interest from the model. As GPT-2 proceeds through the text, it generates a probability distribution (the <code class="docutils literal notranslate"><span class="pre">logits</span></code> extracted below) across all words in the vocabulary with the goal of correctly predicting the next word. We can use this probability distribution to
derive other features of the model’s internal computations. We’ll extract the following features from GPT-2:</p>
<ul class="simple">
<li><p><strong>embeddings</strong>: the 768-dimensional contextual embedding capturing the meaning of the current word</p></li>
<li><p><strong>top_guesses</strong>: the highest probability word GPT-2 predicts for the current word</p></li>
<li><p><strong>ranks</strong>: the rank of the correct word given probabilities across the vocabulary</p></li>
<li><p><strong>true_probs</strong>: the probability at which GPT-2 predicted the current word</p></li>
<li><p><strong>entropies</strong>: how uncertain GPT-2 was about the current word</p>
<ul>
<li><p>low entropy indicates that the probability distribution was “focused” on certain words</p></li>
<li><p>high entropy indicates the probability distribution was more uniform/dispersed across words</p></li>
</ul>
</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">()</span>

<span class="nd">@find_executable_batch_size</span><span class="p">(</span><span class="n">starting_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">inference_loop</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="c1"># nonlocal accelerator  # Ensure they can be used in our context</span>
    <span class="n">accelerator</span><span class="o">.</span><span class="n">free_memory</span><span class="p">()</span>  <span class="c1"># Free all lingering references</span>

    <span class="n">data_dl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="n">top_guesses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">true_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">entropies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_dl</span><span class="p">:</span>
            <span class="c1"># Get output from model</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>
            <span class="n">states</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">hidden_states</span>

            <span class="n">true_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">brange</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_ids</span><span class="p">)))</span>
            <span class="n">logits_order</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">batch_top_guesses</span> <span class="o">=</span> <span class="n">logits_order</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="n">batch_ranks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">logits_order</span><span class="p">,</span> <span class="n">true_ids</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">batch_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">batch_true_probs</span> <span class="o">=</span> <span class="n">batch_probs</span><span class="p">[</span><span class="n">brange</span><span class="p">,</span> <span class="n">true_ids</span><span class="p">]</span>
            <span class="n">batch_entropy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">batch_probs</span><span class="p">)</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
            <span class="n">batch_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span> <span class="p">]</span>

            <span class="n">top_guesses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_top_guesses</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_ranks</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">true_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_true_probs</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_entropy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_embeddings</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">top_guesses</span><span class="p">,</span> <span class="n">ranks</span><span class="p">,</span> <span class="n">true_probs</span><span class="p">,</span> <span class="n">entropies</span><span class="p">,</span> <span class="n">embeddings</span>

<span class="n">top_guesses</span><span class="p">,</span> <span class="n">ranks</span><span class="p">,</span> <span class="n">true_probs</span><span class="p">,</span> <span class="n">entropies</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">inference_loop</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
</pre></div></div>
</div>
<p>Now we will add the additional information from GPT-2 as columns to <code class="docutils literal notranslate"><span class="pre">df</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;true_prob&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">true_probs</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;top_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">top_guesses</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;entropy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">entropies</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word_idx</th>
      <th>word</th>
      <th>start</th>
      <th>end</th>
      <th>hftoken</th>
      <th>token_id</th>
      <th>rank</th>
      <th>true_prob</th>
      <th>top_pred</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Act</td>
      <td>3.710</td>
      <td>3.790</td>
      <td>ĠAct</td>
      <td>2191</td>
      <td>3185</td>
      <td>1.000139e-08</td>
      <td>0</td>
      <td>0.092728</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>Ġone</td>
      <td>530</td>
      <td>46</td>
      <td>2.847577e-03</td>
      <td>352</td>
      <td>5.294118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>one,</td>
      <td>3.990</td>
      <td>4.190</td>
      <td>,</td>
      <td>11</td>
      <td>2</td>
      <td>8.006448e-02</td>
      <td>0</td>
      <td>4.976894</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>monkey</td>
      <td>4.651</td>
      <td>4.931</td>
      <td>Ġmonkey</td>
      <td>21657</td>
      <td>6978</td>
      <td>6.075863e-06</td>
      <td>734</td>
      <td>5.869678</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>in</td>
      <td>4.951</td>
      <td>5.011</td>
      <td>Ġin</td>
      <td>287</td>
      <td>24</td>
      <td>1.004823e-03</td>
      <td>0</td>
      <td>2.478687</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4</td>
      <td>the</td>
      <td>5.051</td>
      <td>5.111</td>
      <td>Ġthe</td>
      <td>262</td>
      <td>0</td>
      <td>3.898537e-01</td>
      <td>262</td>
      <td>4.340655</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>Ġmiddle</td>
      <td>3504</td>
      <td>2</td>
      <td>4.331103e-02</td>
      <td>5228</td>
      <td>5.842120</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5</td>
      <td>middle.</td>
      <td>5.151</td>
      <td>5.391</td>
      <td>.</td>
      <td>13</td>
      <td>3</td>
      <td>4.237065e-02</td>
      <td>286</td>
      <td>2.115351</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>So</td>
      <td>6.592</td>
      <td>6.732</td>
      <td>ĠSo</td>
      <td>1406</td>
      <td>116</td>
      <td>1.016026e-03</td>
      <td>2191</td>
      <td>5.861630</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7</td>
      <td>there's</td>
      <td>6.752</td>
      <td>6.912</td>
      <td>Ġthere</td>
      <td>612</td>
      <td>16</td>
      <td>8.699116e-03</td>
      <td>11</td>
      <td>5.249004</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>And confirm the size and number of embeddings we got. Note that there are 13 layers (instead of the expected 12) because also included are the initial embeddings before the first layer of the network.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2"> layers of embeddings&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Each word embedding is </span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> dimensions long&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
There are 13 layers of embeddings
Each word embedding is 768 dimensions long
</pre></div></div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/neurohack.png" alt="Logo of podcast-ecog-ds"/>
            </a></p>
<h1 class="logo"><a href="../index.html">podcast-ecog-ds</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Tutorials:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-qualitychecks.html">ECoG quality checks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting word embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-encoding.html">Training and evaluating encoding models</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="01-qualitychecks.html" title="previous chapter">ECoG quality checks</a></li>
      <li>Next: <a href="04-encoding.html" title="next chapter">Training and evaluating encoding models</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Zaid Zada.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/notebooks/03-features.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>