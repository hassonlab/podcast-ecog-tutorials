{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces how to extract features, or word embeddings based on our stimulus transcript. Features are numeric vectors that capture the meaning of the words in our transcript. Here, we will extract two types of features: syntactic features from spacy.io ([Honnibal et al., 2020](https://github.com/explosion/spaCy)) and contextual word embeddings from GPT-2 ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).\n",
    "\n",
    "\n",
    "Acknowledgments: This tutorial draws heavily on the [encling tutorial](https://github.com/snastase/encling-tutorial/blob/main/encling_tutorial.ipynb) by Samuel A. Nastase.\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import some general-purpose Python packages. We use `spacy` for syntactic features and [`transformers`](https://huggingface.co/docs/transformers/index) from huggingface for contextual word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs, path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from accelerate import Accelerator, find_executable_batch_size\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Syntactic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will setup the output folder where we will save our syntactic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_syntactic = \"/home/kw1166/scratch/247/monkey-data/stimuli/syntactic\"\n",
    "if not path.exists(output_dir_syntactic):\n",
    "    makedirs(output_dir_syntactic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will load our stimulus transcript as a dataframe. It contains columns of words and their start and end information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act</td>\n",
       "      <td>3.710</td>\n",
       "      <td>3.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one,</td>\n",
       "      <td>3.990</td>\n",
       "      <td>4.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monkey</td>\n",
       "      <td>4.651</td>\n",
       "      <td>4.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>4.951</td>\n",
       "      <td>5.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>5.051</td>\n",
       "      <td>5.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  start    end\n",
       "0     Act  3.710  3.790\n",
       "1    one,  3.990  4.190\n",
       "2  monkey  4.651  4.931\n",
       "3      in  4.951  5.011\n",
       "4     the  5.051  5.111"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_path = \"/home/kw1166/scratch/247/monkey-data/stimuli/monkey_transcript.csv\"\n",
    "df = pd.read_csv(transcript_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract syntactic features, we will use spacy.io ([Honnibal et al., 2020](https://github.com/explosion/spaCy)). First, we will downlod [`en-core-web-lg`](https://spacy.io/models/en#en_core_web_lg), one of the english pipelines from spacy containing tools like tagger and parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the nlp pipeline from spacy by calling `spacy.load()`. We will create a `word_idx` column to keep track of our word and their indices. We will then tokenize the words using the tokenizer from the pipeline, meaning transform each word into a list of tokens. Then, we will [explode](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) the dataframe so that each row of the dataframe is a token. Note that we will add white spaces to the end of words before tokenization so we can track the boundary of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"en_core_web_lg\"\n",
    "nlp = spacy.load(modelname)\n",
    "df.insert(0, \"word_idx\", df.index.values)\n",
    "df[\"word_with_ws\"] = df.word.astype(str) + \" \"\n",
    "try:\n",
    "    df[\"hftoken\"] = df.word_with_ws.apply(nlp.tokenizer)\n",
    "except TypeError:\n",
    "    print(\"typeerror!\")\n",
    "    breakpoint()\n",
    "df = df.explode(\"hftoken\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a [doc object](https://spacy.io/api/doc) (which is a sequence of token objects) from our tokenized text and then pass it through the nlp pipeline to parse it for features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in df.hftoken.tolist()]\n",
    "spaces = [token.whitespace_ == \" \" for token in df.hftoken.tolist()]\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "doc = nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop through the doc, and get the features for each token. The [features](https://spacy.io/usage/linguistic-features#pos-tagging) include `text`, `tag` (detailed part-of-speech tag), `dep` (syntactic dependency, i.e. the relation between tokens), and `is_stop` (is the token part of a stop list, i.e. the most common words of the language?). We will organize the features into a second dataframe and add those columns back to `df`. We will drop the two columns we don't need anymore, and then save `df` for future encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for token in doc:\n",
    "    features.append([token.text, token.tag_, token.dep_, token.is_stop])\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "        features, columns=[\"token\", \"pos\", \"dep\", \"stop\"], index=df.index\n",
    "    )\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "df.drop([\"hftoken\", \"word_with_ws\"], axis=1, inplace=True)\n",
    "df.to_csv(path.join(output_dir_syntactic, \"transcript.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features we get are all categorical, we need to turn them into vectors for encoding. We will use [`LabelBinarizer`](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelBinarizer.html) from sklearn, which fits to all the possible category labels for a feature and then transforms our labels into one-hot vectors. There are 50 possible labels for `tag` and 45 possible for `dep`. So those two features will be turned into 50-dimensional and 45-dimensional vectors respectively. Our `is_stop` feature is binary, so it will just be one dimensional. We concatenate all three features to form a 96-dimensional syntactic feature overall and save it for future encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggerEncoder = LabelBinarizer().fit(nlp.get_pipe(\"tagger\").labels)\n",
    "dependencyEncoder = LabelBinarizer().fit(nlp.get_pipe(\"parser\").labels)\n",
    "\n",
    "a = taggerEncoder.transform(df.pos.tolist())\n",
    "b = dependencyEncoder.transform(df.dep.tolist())\n",
    "c = LabelBinarizer().fit_transform(df.stop.tolist())\n",
    "embeddings = np.hstack((a, b, c))\n",
    "\n",
    "with h5py.File(path.join(output_dir_syntactic, \"states.hdf5\"), \"w\") as f:\n",
    "    f.create_dataset(name=\"vectors\", data=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting GPT-2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract contextual word embeddings from an autoregressive (or \"causal\") large language model (LLM) called GPT-2 ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). GPT-2 relies on the Transformer architecture to sculpt the embedding of a given word based on the preceding context. The model is composed of a repeated circuit motif—called the \"attention head\"—by which the model can \"attend\" to previous words in the context window when determining the meaning of the current word. This GPT-2 implementation is composed of 12 layers, each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 768 features and the context window includes the preceding 1024 tokens. Note that certain words will be broken up into multiple tokens; we'll need to use GPT-2's \"tokenizer\" to convert words into the appropriate tokens. GPT-2 has been (pre)trained on large corpora of text according to a simple self-supervised objective function: predict the next word based on the prior context. If you want to learn more about LLMs and GPT-2, here are some great blogs explaining [transformers](https://jalammar.github.io/illustrated-transformer/) and [GPT-2](https://jalammar.github.io/illustrated-gpt2/) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will setup the output folders where we will save our contextual word embeddings from gpt2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gpt2 = \"/home/kw1166/scratch/247/monkey-data/stimuli/gpt2\"\n",
    "if not path.exists(output_dir_gpt2):\n",
    "    makedirs(output_dir_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will load our stimulus transcript as a dataframe. It contains columns of words and their start and end information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act</td>\n",
       "      <td>3.710</td>\n",
       "      <td>3.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one,</td>\n",
       "      <td>3.990</td>\n",
       "      <td>4.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monkey</td>\n",
       "      <td>4.651</td>\n",
       "      <td>4.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>4.951</td>\n",
       "      <td>5.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>5.051</td>\n",
       "      <td>5.111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  start    end\n",
       "0     Act  3.710  3.790\n",
       "1    one,  3.990  4.190\n",
       "2  monkey  4.651  4.931\n",
       "3      in  4.951  5.011\n",
       "4     the  5.051  5.111"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_path = \"/home/kw1166/scratch/247/monkey-data/stimuli/monkey_transcript.csv\"\n",
    "df = pd.read_csv(transcript_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define some of the general arguments, including the model name as it appears on huggingface, the context length, which is how many tokens we input to the model to extract embeddings (the max value is 1024 for GPT-2, here we will use 32), and device, which is where the model will run. We can set the device to `cuda` to utilize gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"gpt2\"\n",
    "context_len = 32\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the GPT-2 tokenizer to convert words into a list of tokens. Then, we will [explode](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html) the dataframe so that each row of the dataframe is a token. We will convert our tokens to token_ids (which are integers IDs corresponding to words in the GPT-2 vocabulary, which contains approximately 50,000 tokens) and use that as the input to GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "df.insert(0, \"word_idx\", df.index.values)\n",
    "df[\"hftoken\"] = df.word.apply(lambda x: tokenizer.tokenize(\" \" + x))\n",
    "df = df.explode(\"hftoken\", ignore_index=True)\n",
    "df[\"token_id\"] = df.hftoken.apply(tokenizer.convert_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will load the GPT-2 model. We can check its configurations in `model.config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model : gpt2\n",
      "Layers: 12\n",
      "EmbDim: 768\n",
      "Config: GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(modelname)\n",
    "\n",
    "print(\n",
    "    f\"Model : {modelname}\"\n",
    "    f\"\\nLayers: {model.config.num_hidden_layers}\"\n",
    "    f\"\\nEmbDim: {model.config.hidden_size}\"\n",
    "    f\"\\nConfig: {model.config}\"\n",
    ")\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our transcript contains more tokens than the context window (32), we will reformat all the token_ids into `data`, a torch tensor with the shape of (number of tokens * 33). This is because to extract feature for a token from GPT-2 using context length 32, we will need to input 33 tokens to GPT-2, which contains the token itself and the 32 preceding tokens. Note that for the first 32 tokens in the transcript, we will use the pad_token_id or 0 to pad the input length to 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5491, 33])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = df.token_id.tolist()\n",
    "fill_value = 0\n",
    "if tokenizer.pad_token_id is not None:\n",
    "    fill_value = tokenizer.pad_token_id\n",
    "\n",
    "data = torch.full((len(token_ids), context_len + 1), fill_value, dtype=torch.long)\n",
    "for i in range(len(token_ids)):\n",
    "    example_tokens = token_ids[max(0, i - context_len) : i + 1]\n",
    "    data[i, -len(example_tokens) :] = torch.tensor(example_tokens)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [`Accelerator`](https://github.com/huggingface/accelerate) to make extracting features more efficient. It includes a [`find_executable_batch_size`](https://huggingface.co/docs/accelerate/v0.11.0/en/memory) algorithm, which can find the optimal batch size for the code by decreasing the batch size in half after each failed run on the code (in this case, our `inference_loop` function).\n",
    "\n",
    "Inside the `inference_loop` funcion, we will use a PyTorch `DataLoader` to supply token IDs to the model in batches and extract the features. In addition to the embeddings, we'll also extract several other features of potential interest from the model. As GPT-2 proceeds through the text, it generates a probability distribution (the `logits` extracted below) across all words in the vocabulary with the goal of correctly predicting the next word. We can use this probability distribution to derive other features of the model's internal computations. We'll extract the following features from GPT-2:\n",
    "\n",
    "* **embeddings**: the 768-dimensional contextual embedding capturing the meaning of the current word\n",
    "* **top_guesses**: the highest probability word GPT-2 predicts for the current word\n",
    "* **ranks**: the rank of the correct word given probabilities across the vocabulary\n",
    "* **true_probs**: the probability at which GPT-2 predicted the current word\n",
    "* **entropies**: how uncertain GPT-2 was about the current word\n",
    "    * low entropy indicates that the probability distribution was \"focused\" on certain words\n",
    "    * high entropy indicates the probability distribution was more uniform/dispersed across words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "@find_executable_batch_size(starting_batch_size=32)\n",
    "def inference_loop(batch_size=32):\n",
    "    # nonlocal accelerator  # Ensure they can be used in our context\n",
    "    accelerator.free_memory()  # Free all lingering references\n",
    "\n",
    "    data_dl = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "    top_guesses = []\n",
    "    ranks = []\n",
    "    true_probs = []\n",
    "    entropies = []\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_dl:\n",
    "            # Get output from model\n",
    "            output = model(batch.to(device), output_hidden_states=True)\n",
    "            logits = output.logits\n",
    "            states = output.hidden_states\n",
    "\n",
    "            true_ids = batch[:, -1]\n",
    "            brange = list(range(len(true_ids)))\n",
    "            logits_order = logits[:, -2, :].argsort(descending=True)\n",
    "            batch_top_guesses = logits_order[:, 0]\n",
    "            batch_ranks = torch.eq(\n",
    "                logits_order, true_ids.reshape(-1, 1).to(device)\n",
    "            ).nonzero()[:, 1]\n",
    "            batch_probs = torch.softmax(logits[:, -2, :], dim=-1)\n",
    "            batch_true_probs = batch_probs[brange, true_ids]\n",
    "            batch_entropy = torch.distributions.Categorical(\n",
    "                probs=batch_probs\n",
    "            ).entropy()\n",
    "            batch_embeddings = [\n",
    "                state[:, -1, :].numpy(force=True) for state in states\n",
    "            ]\n",
    "\n",
    "            top_guesses.append(batch_top_guesses.numpy(force=True))\n",
    "            ranks.append(batch_ranks.numpy(force=True))\n",
    "            true_probs.append(batch_true_probs.numpy(force=True))\n",
    "            entropies.append(batch_entropy.numpy(force=True))\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        return top_guesses, ranks, true_probs, entropies, embeddings\n",
    "\n",
    "top_guesses, ranks, true_probs, entropies, embeddings = inference_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add the additional information from GPT-2 as columns to `df`. We will then save it for future encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rank\"] = np.concatenate(ranks)\n",
    "df[\"true_prob\"] = np.concatenate(true_probs)\n",
    "df[\"top_pred\"] = np.concatenate(top_guesses)\n",
    "df[\"entropy\"] = np.concatenate(entropies)\n",
    "\n",
    "df.to_csv(path.join(output_dir_gpt2, \"transcript.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will save the contextual word embeddings, which are 798-dimensional features, for future encoding. Note that since GPT-2 has 12 transformer layers, we can extract features from before and after each of the transformer layers. So we will have 13 different versions of GPT-2 features that we could use for encoding. Here, we save them to one file with separate datasets named by the layer index 1~13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path.join(output_dir_gpt2, \"states.hdf5\"), \"w\") as f:\n",
    "    for i in range(len(embeddings[0])):\n",
    "        layer_embeddings = np.vstack([e[i] for e in embeddings])\n",
    "        f.create_dataset(name=f\"layer-{i}\", data=layer_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the features, we are reading for encoding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
